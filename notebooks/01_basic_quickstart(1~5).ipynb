{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: reportlab in /Users/kangjiwon/Desktop/Makina/llm/.venv/lib/python3.10/site-packages (4.4.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/kangjiwon/Desktop/Makina/llm/.venv/lib/python3.10/site-packages (from reportlab) (11.2.1)\n",
      "Requirement already satisfied: chardet in /Users/kangjiwon/Desktop/Makina/llm/.venv/lib/python3.10/site-packages (from reportlab) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from fpdf import FPDF\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLMChain, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LangChain은 언어 체인 혹은 언어 사슬로서, 한 언어에서 다른 언어로의 연결 과정을 나타내는 개념입니다. 이는 특정 언어를 배우기 위해서는 그 언어와 관련된 언어들을 함께 배워야 한다는 것을 의미합니다. 예를 들어, 한국어를 배우기 위해서는 한국어와 관련된 언어인 중국어나 일본어도 함께 배우는 것이 도움이 될 수 있습니다. 따라서 LangChain은 다양한 언어를 연결하여 학습하는 방법을 말합니다. \n"
     ]
    }
   ],
   "source": [
    "# 기본적인 LLM 호출\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "response = llm.invoke(\"LangChain이 뭐야?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나를 창의적인 개발자로 만들어줘.\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 템플릿 활용\n",
    "prompt = PromptTemplate.from_template(\"나를 {adjective} 개발자로 만들어줘.\")\n",
    "print(prompt.format(adjective=\"창의적인\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "그건 나의 능력 밖이지만, 나는 최대한 노력해서 실용적인 개발자로 성장할 수 있도록 도와줄 수 있을 것 같아요. 함께 공부하고 경험을 쌓아가면서 실용적인 스킬을 익히고, 문제를 해결하는 능력을 키우는 것이 중요해요. 그리고 실제 프로젝트를 진행하며 실무에서 필요한 기술과 도구들을 익히는 것도 도움이 될 거에요. 또한, 다양한 분야의 개발에 대한 지식을 넓히고, 문제 해결 능력을 기르는 것도 중요해요. 그리고 끊임없이 배우고 발전하며, 좋은 개발자가 되기 위해 노\n"
     ]
    }
   ],
   "source": [
    "# Runnable Sequence\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\"adjective\": \"실용적인\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 Few-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 동물에 대한 설명입니다.\n",
      "\n",
      "Input: 강아지\n",
      "Output: 귀엽고 털이 많은 포유류\n",
      "\n",
      "Input: 고양이\n",
      "Output: 유연하고 독립적인 성격의 동물\n",
      "\n",
      "Input: 기린\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# 기본 Few-shot Prompt\n",
    "examples = [\n",
    "    {\"input\": \"강아지\", \"output\": \"귀엽고 털이 많은 포유류\"},\n",
    "    {\"input\": \"고양이\", \"output\": \"유연하고 독립적인 성격의 동물\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "fewshot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"다음은 동물에 대한 설명입니다.\",\n",
    "    suffix=\"Input: {animal}\\nOutput:\",\n",
    "    input_variables=[\"animal\"],\n",
    ")\n",
    "\n",
    "print(fewshot_prompt.format(animal=\"기린\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 긴 목과 다양한 무늬를 가진 동물\n"
     ]
    }
   ],
   "source": [
    "# Runnable Sequence\n",
    "chain = fewshot_prompt | llm\n",
    "response = chain.invoke({\"animal\": \"기린\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExampleSelector을 이용한 동적 Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example들이 많을 때, 입력과 유사한 예시만 자동 선택하는 방법\n",
    "- ExampleSelector: abstract base class (ABC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"animal\": \"강아지\", \"desc\": \"사람과 친숙하고 애교 많은 포유류\"},\n",
    "    {\"animal\": \"고양이\", \"desc\": \"독립적이며 유연한 동물\"},\n",
    "    {\"animal\": \"토끼\", \"desc\": \"귀가 길고 점프를 잘함\"},\n",
    "    {\"animal\": \"호랑이\", \"desc\": \"맹수이자 한국의 상징 동물\"},\n",
    "    {\"animal\": \"기린\", \"desc\": \"목이 길고 초식 동물\"},\n",
    "    {\"animal\": \"판다\", \"desc\": \"대나무를 먹는 흑백 털 동물\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"animal\", \"desc\"],\n",
    "    template=\"Input: {animal}\\nOutput: {desc}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Selector\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, # 전체 예시\n",
    "    embedding_model, # 임베딩 방식\n",
    "    Chroma, # 벡터 스토어\n",
    "    k=3, # 몇 개 고를지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 동물 설명 예시입니다.\n",
      "\n",
      "Input: 판다\n",
      "Output: 대나무를 먹는 흑백 털 동물\n",
      "\n",
      "Input: 토끼\n",
      "Output: 귀가 길고 점프를 잘함\n",
      "\n",
      "Input: 기린\n",
      "Output: 목이 길고 초식 동물\n",
      "\n",
      "Input: 사자\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# Few-shot Prompt\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, # examples와 동시에 사용 불가능\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"다음은 동물 설명 예시입니다.\",\n",
    "    suffix=\"Input: {animal}\\nOutput:\",\n",
    "    input_variables=[\"animal\"]\n",
    ")\n",
    "\n",
    "prompt_str = few_shot_prompt.format(animal=\"사자\")\n",
    "print(prompt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Load & Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다양한 형태의 문서를 Document 객체로 load\n",
    "- 문서가 너무 클 경우, LLM에 넘기기 좋게 작은 청크로 분할\n",
    "- RAG의 사전 작업으로 Embdding, Retrieval로 이어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample PDF 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 생성 완료\n"
     ]
    }
   ],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.pdfbase import pdfmetrics\n",
    "from reportlab.pdfbase.ttfonts import TTFont\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_JUSTIFY\n",
    "from reportlab.lib.units import inch\n",
    "\n",
    "# 나눔고딕 폰트 등록\n",
    "pdfmetrics.registerFont(TTFont('NanumGothic', '/Users/kangjiwon/Library/Fonts/NanumGothic-Regular.ttf'))\n",
    "\n",
    "# PDF 문서 설정\n",
    "doc = SimpleDocTemplate(\n",
    "    \"./data/sample.pdf\",\n",
    "    pagesize=letter,\n",
    "    rightMargin=72,  # 1인치\n",
    "    leftMargin=72,   # 1인치\n",
    "    topMargin=72,    # 1인치\n",
    "    bottomMargin=72  # 1인치\n",
    ")\n",
    "\n",
    "# 한글 스타일 정의\n",
    "styles = getSampleStyleSheet()\n",
    "styles.add(ParagraphStyle(\n",
    "    name='Korean',\n",
    "    fontName='NanumGothic',\n",
    "    fontSize=12,\n",
    "    leading=16,      # 줄 간격\n",
    "    alignment=TA_JUSTIFY\n",
    "))\n",
    "\n",
    "# 내용 준비\n",
    "content = []\n",
    "with open(\"./data/sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    # 각 문단을 Paragraph 객체로 변환\n",
    "    paragraphs = text.split('\\n')\n",
    "    for para in paragraphs:\n",
    "        if para.strip():  # 빈 줄 제외\n",
    "            p = Paragraph(para, styles['Korean'])\n",
    "            content.append(p)\n",
    "\n",
    "# PDF 생성\n",
    "doc.build(content)\n",
    "print(\"PDF 생성 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain을 활용한 LLM 기반 시스템 개요\n",
      "LangChain은 다양한 외부 데이터와의 통합, 프롬프트 템플릿 구성, 체인 조합, 그리고\n",
      "메모리 활용 기능을 포함하여 LLM\n"
     ]
    }
   ],
   "source": [
    "# PDF 문서\n",
    "loader = PyPDFLoader(\"./data/sample.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ລາວLietuviųLatviešuМакедонскиമലയാളംМонголमराठीBahasa Melayuမြန်မာဘာသာPlattdüütschनेपालीNederlandsNor\n"
     ]
    }
   ],
   "source": [
    "# 웹 페이지 로딩\n",
    "loader = WebBaseLoader(\"https://ko.wikipedia.org/wiki/파이썬\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[1400:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain을 활용한 LLM 기반 시스템 개요\n",
      "\n",
      "LangChain은 다양한 외부 데이터와의 통합, 프롬프트 템플릿 구성, 체인 조합, 그리고 메모리 활용 기능을 포함하여 LL\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 문서 로딩\n",
    "loader = TextLoader(\"./data/sample.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 chunks created\n",
      "Chunk 1: LangChain을 활용한 LLM...\n",
      "Chunk 2: LLM 기반 시스템 개요...\n",
      "Chunk 3: LangChain은 다양한 외부...\n",
      "Chunk 4: 외부 데이터와의 통합, 프롬프트...\n",
      "Chunk 5: 프롬프트 템플릿 구성, 체인 조합,...\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter: 문단 -> 줄바꿈 -> 문장 -> 단어 -> 문자 순으로 Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"{len(chunks)} chunks created\")\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i+1}: {chunk.page_content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding & Vector-store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding들을 Vectore-store에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/sample.pdf\")\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=5\n",
    ")\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 108개의 벡터가 저장됨\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./vector_db\",\n",
    ")\n",
    "\n",
    "print(f\"총 {vectorstore._collection.count()}개의 벡터가 저장됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사한 문서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[결과 1]: LLM 기반 시스템 개요\n",
      "[결과 2]: LLM 기반 시스템 개요\n",
      "[결과 3]: LLM 기반 시스템 개요\n"
     ]
    }
   ],
   "source": [
    "query = \"LLM 시스템의 핵심 구성요소는?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"[결과 {i+1}]: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore 저장 & 재사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./vector_db\",\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieval + QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 구성\n",
    "template = \"\"\"너는 친절한 LLM 도우미야. 다음 context를 참고해서 질문에 답변해줘.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever는 Document 객체의 리스트를 반환하지만 LLM은 문자열 입력을 기대\n",
    "# 각 Document를 page_content로 추출하고 이를 결합하여 하나의 문자열로 만듬\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL 방식의 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 답변: LangChain의 핵심 구성 요소는 다양한 외부 데이터 소스와 연결되어 있는 언어 모델이다.\n"
     ]
    }
   ],
   "source": [
    "# LCEL (LangChain Expression Language) 사용\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n",
    "\n",
    "query = \"LangChain의 핵심 구성 요소는 뭐야?\"\n",
    "response = rag_chain.invoke(query)\n",
    "print(f\"LLM 답변: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 단계를 명시적으로 구현한 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 답변: LangChain의 핵심 구성 요소는 다양한 외부 데이터 소스와 연결되어 있는 언어 모델이다.\n"
     ]
    }
   ],
   "source": [
    "# 실제 작동 방식\n",
    "docs = retriever.invoke(query) # [Document1, Document2, ...]\n",
    "context = format_docs(docs) # \"Document1의 내용\\n\\nDocument2의 내용\\n\\n...\"\n",
    "\n",
    "inputs = {\n",
    "    \"context\": context,\n",
    "    \"question\": query\n",
    "}\n",
    "\n",
    "formatted_prompt = prompt.format(**inputs)\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"LLM 답변: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출처 문서 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableMap\n",
    "- 병렬 실행, 스트리밍, 비동기 실행 등 지원\n",
    "- 에러 핸들링 내장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 초기화\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 벡터 DB 생성 또는 로드\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./vector_db\",\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "# retriever 생성\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PromptTemplate: 단순 텍스트 출력\n",
    "- ChatPromptTemplate: 메세지 역할 (system, user, assistant) 구분 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"다음 컨텍스트를 사용하여 질문에 답변해주세요.\n",
    "\n",
    "컨텍스트: {context}\n",
    "\n",
    "질문: {question}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"question\": \"LangChain의 핵심 구성 요소는 뭐야?\", \"language\": \"ko\"}\n",
    "\n",
    "# 1단계: 문서 검색\n",
    "context_chain = itemgetter(\"question\") | retriever # retriever에 질문 텍스트만 전달\n",
    "\n",
    "# 2단계: 답변 생성\n",
    "rag_chain = {\n",
    "    \"context\": context_chain | format_docs,\n",
    "    \"question\": itemgetter(\"question\") # 프롬프트 템플릿에 원본 질문 제공\n",
    "} | prompt | llm\n",
    "\n",
    "# 3단계: 전체 결과 구성\n",
    "full_chain = RunnableMap({\n",
    "    \"result\": rag_chain,\n",
    "    \"source_documents\": context_chain\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sample.pdf\n",
      "LangChain을 활용한 LLM\n",
      "./data/sample.pdf\n",
      "LangChain을 활용한 LLM\n",
      "./data/sample.pdf\n",
      "LangChain을 활용한 LLM\n",
      "./data/sample.pdf\n",
      "LangChain은 다양한 외부\n"
     ]
    }
   ],
   "source": [
    "output = full_chain.invoke(query)\n",
    "for doc in output[\"source_documents\"]:\n",
    "    print(doc.metadata[\"source\"]) # 문서 출처\n",
    "    print(doc.page_content) # 문서 내용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
